### 组件

- Engine：引擎负责控制数据流在系统中所有组件中流动，并在相应的动作发生时触发
- Scheduler：调度器从引擎接收Request并将他们入队，以便之后引擎请求他们时提供的引擎
- DownLoader：下载器负责获取页面数据并提供给引擎，而后提供给Spider
- Spider：Spider是Scrapy用户编写的用于分析Response并提取Item或者提取更多需要下载的URL的类。每个Spider负责处理特定的网站
- Item PipeLine：负责处理被Spider提取出来的Item。典型的功能有清晰、验证及持久化操作
- Downloader middlewares：下载器中间件是在Engine及Downloader之间的特定钩子（specific hooks），处理Downloader传递给Engine的Response。其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。
- Spider middlewares：是在Engine及Spider之间的特定钩子（specific hooks），处理Spider的输入（Response）和输出（Items及Request）。其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。


### 数据流

1. Engine从Spider获取第一个需要爬取的URL（s）
2. Engine用Scheduler调度Requests，并向Scheduler请求下一个要爬取的URL
3. Scheduler返回下一个需要爬取的URL给Engine
4. Engine将URL通过Downloader middlewares转发给Downloader
5. 一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过Downloader middlewares发送给Engine
6. 引擎从Downloader中接收Response并通过Spider middlewares发送给Spider处理
7. Spider处理Response并返回爬取到的Item及新的Request给Engine
8. Engine将爬取到的Item给Item Pipeline，然后将Request给Scheduler
9. 从第一步开始重复这个流程，知道Scheduler中没有更多的URLs